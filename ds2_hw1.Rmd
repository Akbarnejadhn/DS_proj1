---
title: "Practicing Linear, Ridge, Lasso, PC Regression models"
author: "Hana Akbarnejad"
date: "2/27/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)


library(tidyverse)
library(viridis)
library(ggplot2)
library(readr)
library(modelr)
library(ISLR)
library(glmnet)
library(caret)
library(corrplot)
library(plotmo)
library(pls)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

```{r import_data, include=FALSE}

train_data = read_csv("solubility_train.csv") %>% 
  janitor::clean_names() %>% 
  na.omit()

test_data = read_csv("solubility_test.csv") %>% 
  janitor::clean_names() %>% 
  na.omit()
```

In this exercise, we will predict solubility of compounds using their chemical structures. We will use traing and test data, fit linear, Ridge, Lasso, and principle component regression models and identify the best mosel that can be used for predicting solubility.

After importing data and omitting the missing values, we should start by defining test and train model matrices for predictors and also define the responses in train and test data. We will later need them for our models in this exercise. I also defined a *train_control* that uses the repeated cross validation method, with 50 resamples (5 times, 10 each).

```{r}

set.seed(1)

# setting up x matrix and y for train and test data
x_train = model.matrix(solubility ~ ., train_data)[,-1]
y_train = train_data$solubility

x_test = model.matrix(solubility ~ ., test_data)[,-1]
y_test = test_data$solubility

# need to define a train control, reampling method specified as repeated Cv
# tuneGrid: candidates for tuning parameter
# I chose to pre-process!
# "train" here is resampling method and is different from the next train :)
# I will use it every where needed!

train_control = trainControl(method = "repeatedcv", number = 10, repeats = 5)

```


### (a)

In the first part, we are interested in fitting a linear model using least squares on the training data and calculate the mean square error using the test data.

```{r lm}

set.seed(1)

# fit lm model using training data
lm_fit = train(x_train, y_train,
              method = "lm",
              trControl = train_control)

# predict model using training model and test data
test_pred = predict(lm_fit, test_data)
summary(test_pred)

# calculating MSE using training and test models
# mean((y_pred - y_test)^2)
mse = mean((test_pred - test_data$solubility)^2)
```

After fitting a linear model and using it to predict the test data,we obtain mean square error of `r round(mse, 3)`.

### (b)

In this part, we want to choose $\lambda$ and fit a ridge regression model on the training data and find out the test error.

We first fit a ridge regression model. We use cross validation to choose the optimal value of $\lambda$. We use $\alpha=0$ which is the ridge penalty to obtain $\lambda$ using *cv.glmnet* function. Alternatively, we can Use *train()* function from *caret* package to fit Ridge regression:

```{r ridge}

set.seed(1)

ridge_fit = train(x_train, y_train,
                  method = "glmnet",
                  tuneGrid = expand.grid(alpha = 0,
                                         lambda = exp(seq(-5, 5, length = 100))),
            preProc = c("center", "scale"),
            trControl = train_control)

# plot the parameters over RMSE to choose the best option
plot(ridge_fit, xTrans = function(x) log(x))

# best lambda!
ridge_lambda = ridge_fit$bestTune

# summary of coefficients in Ridge regression
# coef(ridge_fit$finalModel,ridge_fit$bestTune$lambda)

# MSE from Ridge
pred_ridge_fit = predict(ridge_fit, newdata = test_data)
ridge_mse = mean((pred_ridge_fit - y_test)^2)

```

Uisng Ridge regression, we can see that the optimum $\lambda$ chosen is `r round(ridge_fit$bestTune, 3)[1,2]`.

To compute the test error, we need to predict the test data using the model we have built and then use that to compute the MSE. We can see that the test error obtained from this model is `r round(ridge_mse, 3)`.

### (c)

In this part, we want to choose $\lambda$ by cross-validation and fit a Lasso regression model on the training data and find the test error.

```{r lasso}

set.seed(1)
# fitting lasso regression, we should set alpha as 1
lasso_fit = train(x_train, y_train,
                  method = "glmnet",
                  tuneGrid = expand.grid(alpha = 1,
                                         lambda = exp(seq(-10, 2, length = 100))),
            preProc = c("center", "scale"),
            trControl = train_control)

# plotting
plot(lasso_fit, xTrans = function(x) log(x))

# best lambda!
lasso_lambda = lasso_fit$bestTune

# summary of coefficients in Lasso regression
lasso_coeff = coef(lasso_fit$finalModel,lasso_fit$bestTune$lambda)

# MSE from Lasso
pred_lasso_fit = predict(lasso_fit, newdata = test_data)
lasso_mse = mean((pred_lasso_fit - y_test)^2)

# non-zero coefficients
coeff_df = as.data.frame(as.matrix(lasso_coeff))

nonzero_coeff = coeff_df %>% 
  filter(coeff_df[1] != 0) %>% 
  nrow()
```

Using the fitted Lasso model, cross validation and the plot above that depicts RMSE against tuning parameters, We can see that the $\lambda$ obtained from cross validation is `r round(lasso_lambda, 3)[1, 2]` using $\alpha = 1$, the test error is `r round(lasso_mse, 3)`, and that the number of non-zero parameters is `r nonzero_coeff`.

### (d)

In this part, we want to choose M by cross-validation, and fit a principle component regression model on the training data. We furthur want to compute the test error:

```{r pcr}

set.seed(1)

# fitting PCR
pcr_fit = train(x_train, y_train, method = "pcr",
                tuneGrid = data.frame(ncomp = 1:226), # it should be from 1 to the number of predictors (228), but then I get a 
                trControl = train_control,            # wired results when I knit, so we try to omit the problematic point
                preProc = c("center", "scale"))

trans = preProcess(x_train, method = c("center", "scale"))

pred_pcr = predict(pcr_fit$finalModel, newdata = predict(trans, x_test),
                   ncomp = pcr_fit$bestTune$ncomp)

pcr_mse = mean((y_test - pred_pcr)^2)

pcr_m = pcr_fit$bestTune

ggplot(pcr_fit, highlight = TRUE) + theme_bw()

```

We can observe that using principal components regression, the value of M chosen by cross validation is `r pcr_m` which can also be observed on the plot above, and the test error is `r round(pcr_mse, 3)`.

### (e)

The above analysis show that using seed 1 and on this computer(!), MSE obtained from Lasso Regression is `r round(lasso_mse, 3)`, MSE from Ridge regression is `r round(ridge_mse, 3)` and the MSE from PCR is `r round(pcr_mse, 3)`. The MSE from the linear regression is the highest among these four models with the value of `r round(mse, 3)`.

The $(\alpha,\lambda)$ parameters from Ridge and Lasso were (`r round(ridge_fit$bestTune, 3)`) and (`r round(lasso_fit$bestTune, 3)`), respectively, and the M from PCR is `r pcr_m`.

lambda for lasso much lower than ridge (0.004 vs 0.126).

### (f)

```{r}

resamp = resamples(list(pcr = pcr_fit, lasso = lasso_fit, ridge = ridge_fit, lm = lm_fit))
summary(resamp) # comment on rmse
```

In this section we want to select one model which is the best between linear, Ridge, Lasso, and PC regressions that can be used for predicting solubility. For this purpose, we review MSE values discissed in previous part and see that deciding between these models, Lasso offers the best model with the minimum error and that the linear regression is not complex enough to cover our datapoints in a proper way.

we apply *resamples()* function on these models and look at the mean in *RMSE* results. We can see that RMSE for linear model is the higher with the value of 0.711, the second highest mean RMSE is for the PCR model with 0.707, then it is Ridge with 0.687 and finally Lasso, which has the lowest RMSE with the value of 0.678. These results confirm that Lasso is a better fit to our data compared to the other three models and is the model I would choose for analyzing this dataset to predict the solubility.

Below, we can also see a parallel plot and boxplots of the methods we used that can help visualize the results we discussed.

```{r rmse_plots}

parallelplot(resamp, metric = "RMSE")
bwplot(resamp, metric = "RMSE")
```

